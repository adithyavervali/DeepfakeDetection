{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37114348-b82d-4813-9084-b8bdcd9ef850",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (2415137539.py, line 25)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mBASE_DIR = \"C:\\Users\\Admin\\deepfake\"\u001b[39m\n                                         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DEEPFAKE DETECTION - LOCAL DESKTOP/VS CODE VERSION\n",
    "# ============================================================\n",
    "# This version is adapted for running on your local machine with GPU\n",
    "# No Google Drive mounting needed!\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import copy\n",
    "import sys\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "sys.stdout.flush()\n",
    "\n",
    "# ---------------- LOCAL PATHS CONFIG ----------------\n",
    "# CHANGE THESE PATHS TO YOUR LOCAL DIRECTORIES\n",
    "BASE_DIR = r\"C:\\Users\\Admin\\deepfake\"\n",
    "\n",
    "\n",
    "train_dirs = [\n",
    "    (os.path.join(BASE_DIR, \"train/real\"), None, 0),\n",
    "    (os.path.join(BASE_DIR, \"train/fake\"), None, 1)\n",
    "]\n",
    "valid_dirs = [\n",
    "    (os.path.join(BASE_DIR, \"valid/real\"), None, 0),\n",
    "    (os.path.join(BASE_DIR, \"valid/fake\"), None, 1)\n",
    "]\n",
    "test_image_dir = os.path.join(BASE_DIR, \"test/images\")\n",
    "test_video_dir = os.path.join(BASE_DIR, \"test/videos\")\n",
    "models_dir = os.path.join(BASE_DIR, \"models\")\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# ---------------- TRAINING CONFIG ----------------\n",
    "sequence_length = 4\n",
    "batch_size = 8  # Increase if you have good GPU (GTX 1080 or better)\n",
    "num_epochs = 25\n",
    "learning_rate = 2e-5\n",
    "patience = 4\n",
    "min_delta = 0.005\n",
    "\n",
    "# GPU Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üî• GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected! Training will be VERY slow on CPU.\")\n",
    "    print(\"   Consider using Google Colab if you don't have a GPU.\")\n",
    "\n",
    "# ---------------- PATH VALIDATION ----------------\n",
    "print(\"\\nüìÅ Checking directories...\")\n",
    "required_dirs = {\n",
    "    \"Train Real\": train_dirs[0][0],\n",
    "    \"Train Fake\": train_dirs[1][0],\n",
    "    \"Valid Real\": valid_dirs[0][0],\n",
    "    \"Valid Fake\": valid_dirs[1][0],\n",
    "}\n",
    "\n",
    "all_exist = True\n",
    "for name, path in required_dirs.items():\n",
    "    exists = os.path.exists(path)\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"   {status} {name}: {path}\")\n",
    "    if exists:\n",
    "        file_count = len([f for f in os.listdir(path) if f.lower().endswith(('.jpg', '.png', '.jpeg'))])\n",
    "        print(f\"      ‚îî‚îÄ {file_count} images\")\n",
    "    else:\n",
    "        all_exist = False\n",
    "\n",
    "if not all_exist:\n",
    "    print(\"\\n‚ùå ERROR: Some required directories are missing!\")\n",
    "    print(f\"\\nPlease create the directory structure at: {BASE_DIR}\")\n",
    "    print(\"   Deepfake/\")\n",
    "    print(\"   ‚îú‚îÄ‚îÄ train/real/\")\n",
    "    print(\"   ‚îú‚îÄ‚îÄ train/fake/\")\n",
    "    print(\"   ‚îú‚îÄ‚îÄ valid/real/\")\n",
    "    print(\"   ‚îî‚îÄ‚îÄ valid/fake/\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ---------------- LOAD CLIP MODEL ----------------\n",
    "print(\"\\nüì¶ Loading CLIP model...\")\n",
    "try:\n",
    "    clip_model_base, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        \"ViT-B-16\", pretrained=\"openai\", device=device\n",
    "    )\n",
    "    print(\"‚úÖ CLIP model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading CLIP: {e}\")\n",
    "    print(\"Run: pip install open_clip_torch\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ---------------- TRANSFORMS ----------------\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.3),\n",
    "    preprocess.transforms[-2],\n",
    "    preprocess.transforms[-1]\n",
    "])\n",
    "test_transform = preprocess\n",
    "\n",
    "# ---------------- DATASET ----------------\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, datasets, transform=None, seq_len=8, debug=False):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        self.seq_len = seq_len\n",
    "        self.debug = debug\n",
    "        \n",
    "        real_count = 0\n",
    "        fake_count = 0\n",
    "\n",
    "        for img_dir, _, label in datasets:\n",
    "            if not os.path.exists(img_dir):\n",
    "                print(f\"‚ö†Ô∏è Missing directory: {img_dir}\")\n",
    "                continue\n",
    "\n",
    "            img_files = sorted([\n",
    "                f for f in os.listdir(img_dir)\n",
    "                if os.path.isfile(os.path.join(img_dir, f)) and f.lower().endswith(('.jpg', '.png', '.jpeg'))\n",
    "            ])\n",
    "            \n",
    "            if len(img_files) < seq_len:\n",
    "                print(f\"‚ö†Ô∏è Not enough images in {img_dir}: {len(img_files)} < {seq_len}\")\n",
    "                continue\n",
    "\n",
    "            for i in range(0, len(img_files) - seq_len + 1, seq_len):\n",
    "                seq_paths = [os.path.join(img_dir, img_files[j]) for j in range(i, i + seq_len)]\n",
    "                if all(os.path.exists(p) for p in seq_paths):\n",
    "                    self.samples.append((seq_paths, label))\n",
    "                    if label == 0:\n",
    "                        real_count += 1\n",
    "                    else:\n",
    "                        fake_count += 1\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"\\nüìä Dataset Distribution:\")\n",
    "            print(f\"   Real sequences: {real_count}\")\n",
    "            print(f\"   Fake sequences: {fake_count}\")\n",
    "            print(f\"   Total sequences: {len(self.samples)}\")\n",
    "            if real_count > 0 and fake_count > 0:\n",
    "                ratio = max(real_count, fake_count) / min(real_count, fake_count)\n",
    "                print(f\"   Class imbalance ratio: {ratio:.2f}:1\")\n",
    "                if ratio > 3:\n",
    "                    print(f\"   ‚ö†Ô∏è WARNING: Severe class imbalance detected!\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame_paths, label = self.samples[idx]\n",
    "        frames = []\n",
    "        for p in frame_paths:\n",
    "            try:\n",
    "                img = Image.open(p).convert(\"RGB\")\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                frames.append(img)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {p}: {e}\")\n",
    "                frames.append(torch.zeros(3, 224, 224))\n",
    "        \n",
    "        frames = torch.stack(frames)\n",
    "        return frames, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# ---------------- MODEL ----------------\n",
    "class CLIP_EfficientNet_LSTM(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes=2, hidden_dim=512, lstm_layers=2, bidirectional=True, device='cpu'):\n",
    "        super(CLIP_EfficientNet_LSTM, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.clip = clip_model.visual.to(device)\n",
    "        for p in self.clip.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        effnet = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.IMAGENET1K_V1)\n",
    "        effnet = effnet.to(device).eval()\n",
    "        self.efficientnet = nn.Sequential(*list(effnet.children())[:-2])\n",
    "        self.efficientnet_avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        for p in self.efficientnet.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        clip_latent_dim = 512\n",
    "        effnet_latent_dim = 1536\n",
    "        fused_dim = clip_latent_dim + effnet_latent_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=fused_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=0.3\n",
    "        )\n",
    "        lstm_out_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LayerNorm(lstm_out_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(lstm_out_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B * T, C, H, W)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            clip_features = self.clip(x)\n",
    "            effnet_features = self.efficientnet_avgpool(self.efficientnet(x))\n",
    "            effnet_features = effnet_features.view(B * T, -1)\n",
    "\n",
    "        clip_features = clip_features.view(B, T, -1)\n",
    "        effnet_features = effnet_features.view(B, T, -1)\n",
    "        fused = torch.cat((clip_features, effnet_features), dim=2)\n",
    "\n",
    "        lstm_out, _ = self.lstm(fused)\n",
    "        x_mean = torch.mean(lstm_out, dim=1)\n",
    "        x_max, _ = torch.max(lstm_out, dim=1)\n",
    "        x = x_mean + x_max\n",
    "        \n",
    "        return self.fc(x)\n",
    "\n",
    "# ---------------- EARLY STOPPING ----------------\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=4, min_delta=0.005, mode='max'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            return False\n",
    "        \n",
    "        if self.mode == 'max':\n",
    "            improved = score > (self.best_score + self.min_delta)\n",
    "        else:\n",
    "            improved = score < (self.best_score - self.min_delta)\n",
    "        \n",
    "        if improved:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "            return self.early_stop\n",
    "\n",
    "# ---------------- LOAD DATA ----------------\n",
    "print(\"\\nüìÇ Loading datasets...\")\n",
    "train_dataset = SequenceDataset(train_dirs, transform=train_transform, seq_len=sequence_length, debug=True)\n",
    "valid_dataset = SequenceDataset(valid_dirs, transform=test_transform, seq_len=sequence_length, debug=True)\n",
    "\n",
    "if len(train_dataset) == 0:\n",
    "    print(\"‚ùå ERROR: No training data found!\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Weighted sampling\n",
    "labels = [label for _, label in train_dataset.samples]\n",
    "class_counts = [labels.count(0), labels.count(1)]\n",
    "\n",
    "if class_counts[0] == 0 or class_counts[1] == 0:\n",
    "    print(\"‚ùå ERROR: One class has no samples!\")\n",
    "    sys.exit(1)\n",
    "\n",
    "total_samples = len(labels)\n",
    "num_classes = 2\n",
    "class_weights = [total_samples / (num_classes * count) for count in class_counts]\n",
    "sample_weights = [class_weights[l] for l in labels]\n",
    "\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# Adjust num_workers for your system (0-4 is typical for desktop)\n",
    "num_workers = 2 if os.name == 'nt' else 4  # Windows uses fewer workers\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, \n",
    "                          num_workers=num_workers, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, \n",
    "                          num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loaded successfully\")\n",
    "print(f\"   Train batches: {len(train_loader)} | Val batches: {len(valid_loader)}\")\n",
    "print(f\"   Num workers: {num_workers}\")\n",
    "\n",
    "# ---------------- TRAINING SETUP ----------------\n",
    "print(\"\\nüèóÔ∏è Building model...\")\n",
    "model = CLIP_EfficientNet_LSTM(clip_model_base, device=device).to(device)\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=0.1)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), \n",
    "    lr=learning_rate, \n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=2, verbose=True\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(patience=patience, min_delta=min_delta, mode='max')\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "# Training log file\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = os.path.join(models_dir, f\"training_log_{timestamp}.txt\")\n",
    "\n",
    "def log_print(message):\n",
    "    \"\"\"Print and save to log file\"\"\"\n",
    "    print(message)\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(message + '\\n')\n",
    "\n",
    "log_print(f\"\\nüöÄ Training Configuration:\")\n",
    "log_print(f\"   Device: {device}\")\n",
    "log_print(f\"   Max Epochs: {num_epochs}\")\n",
    "log_print(f\"   Batch Size: {batch_size}\")\n",
    "log_print(f\"   Learning Rate: {learning_rate}\")\n",
    "log_print(f\"   Patience: {patience}\")\n",
    "log_print(f\"   Log file: {log_file}\")\n",
    "log_print(\"=\" * 80)\n",
    "\n",
    "# ---------------- TRAINING LOOP ----------------\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        log_print(f\"\\nüìç EPOCH {epoch+1}/{num_epochs}\")\n",
    "        log_print(\"-\" * 80)\n",
    "\n",
    "        # ============ TRAINING ============\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        class_correct = [0, 0]\n",
    "        class_total = [0, 0]\n",
    "\n",
    "        train_pbar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "\n",
    "        for batch_idx, (frames, labels_) in enumerate(train_pbar):\n",
    "            frames, labels_ = frames.to(device), labels_.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(frames)\n",
    "            loss = criterion(outputs, labels_)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels_).sum().item()\n",
    "            total += labels_.size(0)\n",
    "            \n",
    "            for i in range(len(labels_)):\n",
    "                label = labels_[i].item()\n",
    "                class_total[label] += 1\n",
    "                if preds[i] == labels_[i]:\n",
    "                    class_correct[label] += 1\n",
    "\n",
    "            current_acc = 100 * correct / total\n",
    "            train_pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{current_acc:.2f}%'\n",
    "            })\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_real_acc = 100 * class_correct[0] / class_total[0] if class_total[0] > 0 else 0\n",
    "        train_fake_acc = 100 * class_correct[1] / class_total[1] if class_total[1] > 0 else 0\n",
    "\n",
    "        # ============ VALIDATION ============\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        val_class_correct = [0, 0]\n",
    "        val_class_total = [0, 0]\n",
    "        all_probs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for frames, labels_ in tqdm(valid_loader, desc=\"Validating\", leave=False):\n",
    "                frames, labels_ = frames.to(device), labels_.to(device)\n",
    "                outputs = model(frames)\n",
    "                loss = criterion(outputs, labels_)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                val_total += labels_.size(0)\n",
    "                val_correct += (preds == labels_).sum().item()\n",
    "                \n",
    "                for i in range(len(labels_)):\n",
    "                    label = labels_[i].item()\n",
    "                    val_class_total[label] += 1\n",
    "                    if preds[i] == labels_[i]:\n",
    "                        val_class_correct[label] += 1\n",
    "                \n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "        val_acc = 100 * val_correct / val_total if val_total > 0 else 0\n",
    "        avg_val_loss = val_loss / len(valid_loader) if len(valid_loader) > 0 else 0\n",
    "        val_real_acc = 100 * val_class_correct[0] / val_class_total[0] if val_class_total[0] > 0 else 0\n",
    "        val_fake_acc = 100 * val_class_correct[1] / val_class_total[1] if val_class_total[1] > 0 else 0\n",
    "        avg_confidence = np.mean([max(p) for p in all_probs]) if all_probs else 0\n",
    "\n",
    "        scheduler.step(val_acc)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # ============ LOGGING ============\n",
    "        log_print(f\"\\nüìä Epoch {epoch+1} Results:\")\n",
    "        log_print(f\"   Train Loss: {avg_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        log_print(f\"     ‚îú‚îÄ Real: {train_real_acc:.2f}% | Fake: {train_fake_acc:.2f}%\")\n",
    "        log_print(f\"   Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "        log_print(f\"     ‚îú‚îÄ Real: {val_real_acc:.2f}% | Fake: {val_fake_acc:.2f}%\")\n",
    "        log_print(f\"     ‚îî‚îÄ Avg Confidence: {avg_confidence:.4f}\")\n",
    "        log_print(f\"   Learning Rate: {current_lr:.2e}\")\n",
    "\n",
    "        # ============ SAVE BEST MODEL ============\n",
    "        is_best = val_acc > best_val_acc\n",
    "        \n",
    "        if is_best:\n",
    "            improvement = val_acc - best_val_acc\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch + 1\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            log_print(f\"   ‚ú® NEW BEST! (+{improvement:.2f}%)\")\n",
    "        \n",
    "        # ============ EARLY STOPPING ============\n",
    "        if early_stopping(val_acc):\n",
    "            log_print(f\"\\n‚èπÔ∏è EARLY STOPPING TRIGGERED!\")\n",
    "            log_print(f\"   No improvement for {patience} epochs\")\n",
    "            log_print(f\"   Best Val Acc: {best_val_acc:.2f}% (Epoch {best_epoch})\")\n",
    "            break\n",
    "        \n",
    "        if early_stopping.counter > 0:\n",
    "            log_print(f\"   ‚è≥ Patience: {early_stopping.counter}/{patience}\")\n",
    "\n",
    "        if avg_confidence < 0.6:\n",
    "            log_print(f\"   ‚ö†Ô∏è WARNING: Low average confidence ({avg_confidence:.2f})\")\n",
    "\n",
    "        log_print(\"=\" * 80)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    log_print(\"\\n‚ö†Ô∏è Training interrupted by user!\")\n",
    "    log_print(\"   Saving current best model...\")\n",
    "\n",
    "# ---------------- SAVE MODEL ----------------\n",
    "log_print(\"\\nüíæ Saving best model...\")\n",
    "model.load_state_dict(best_model_wts)\n",
    "save_path = os.path.join(models_dir, f\"clip_effnet_lstm_best_{timestamp}.pth\")\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'best_val_acc': best_val_acc,\n",
    "    'best_epoch': best_epoch,\n",
    "    'config': {\n",
    "        'sequence_length': sequence_length,\n",
    "        'hidden_dim': 512,\n",
    "        'lstm_layers': 2,\n",
    "        'bidirectional': True\n",
    "    },\n",
    "    'timestamp': timestamp\n",
    "}, save_path)\n",
    "\n",
    "log_print(f\"‚úÖ Model saved to: {save_path}\")\n",
    "log_print(f\"üèÜ Best Validation Accuracy: {best_val_acc:.2f}% (Epoch {best_epoch})\")\n",
    "\n",
    "# ---------------- TESTING ----------------\n",
    "if os.path.exists(test_image_dir):\n",
    "    log_print(f\"\\nüß™ Testing on images from: {test_image_dir}\")\n",
    "    model.eval()\n",
    "    classes = [\"Real\", \"Fake\"]\n",
    "    \n",
    "    img_files = sorted([f for f in os.listdir(test_image_dir) \n",
    "                       if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "\n",
    "    if len(img_files) > 0:\n",
    "        log_print(f\"Found {len(img_files)} test images\\n\")\n",
    "        \n",
    "        for img_file in img_files[:20]:\n",
    "            try:\n",
    "                img_path = os.path.join(test_image_dir, img_file)\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                img_tensor = test_transform(img)\n",
    "                seq_tensor = img_tensor.unsqueeze(0).repeat(sequence_length, 1, 1, 1).unsqueeze(0).to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(seq_tensor)\n",
    "                    probs = torch.softmax(outputs, dim=1)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    confidence = probs[0, preds.item()].item()\n",
    "                    \n",
    "                    if confidence > 0.75:\n",
    "                        conf_emoji = \"üü¢\"\n",
    "                    elif confidence > 0.60:\n",
    "                        conf_emoji = \"üü°\"\n",
    "                    else:\n",
    "                        conf_emoji = \"üî¥\"\n",
    "                    \n",
    "                    result = f\"{conf_emoji} {img_file} ‚Üí {classes[preds.item()]} (conf: {confidence:.4f})\"\n",
    "                    log_print(result)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                log_print(f\"Error processing {img_file}: {e}\")\n",
    "\n",
    "log_print(\"\\n‚úÖ Training complete!\")\n",
    "log_print(f\"üìÑ Full log saved to: {log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2993001b-efad-41d5-9444-90c748afbccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
