{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37114348-b82d-4813-9084-b8bdcd9ef850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No GPU detected! Training will be VERY slow on CPU.\n",
      "          Consider using Google Colab if you don't have a GPU.\n",
      "\n",
      "[CHECK] Checking directories...\n",
      "   [OK] Train Real: C:\\Users\\Admin\\deepfake\\train/real\n",
      "         1318 images\n",
      "   [OK] Train Fake: C:\\Users\\Admin\\deepfake\\train/fake\n",
      "         24 images\n",
      "   [OK] Valid Real: C:\\Users\\Admin\\deepfake\\valid/real\n",
      "         657 images\n",
      "   [OK] Valid Fake: C:\\Users\\Admin\\deepfake\\valid/fake\n",
      "         0 images\n",
      "\n",
      "[LOAD] Loading CLIP model...\n",
      "[OK] CLIP model loaded successfully\n",
      "\n",
      "[LOAD] Loading datasets...\n",
      "\n",
      "[DATA] Dataset Distribution:\n",
      "       Real sequences: 329\n",
      "       Fake sequences: 6\n",
      "       Total sequences: 335\n",
      "       Class imbalance ratio: 54.83:1\n",
      "       [WARNING] Severe class imbalance detected!\n",
      "[WARNING] Not enough images in C:\\Users\\Admin\\deepfake\\valid/fake: 0 < 4\n",
      "\n",
      "[DATA] Dataset Distribution:\n",
      "       Real sequences: 164\n",
      "       Fake sequences: 0\n",
      "       Total sequences: 164\n",
      "\n",
      "[OK] Dataset loaded successfully\n",
      "     Train batches: 42 | Val batches: 21\n",
      "     Num workers: 2\n",
      "\n",
      "[BUILD] Building model...\n",
      "\n",
      "[TRAIN] Training Configuration:\n",
      "        Device: cpu\n",
      "        Max Epochs: 25\n",
      "        Batch Size: 8\n",
      "        Learning Rate: 2e-05\n",
      "        Patience: 4\n",
      "        Log file: C:\\Users\\Admin\\deepfake\\models\\training_log_20251023_171335.txt\n",
      "================================================================================\n",
      "\n",
      "[EPOCH] 1/25\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                 | 0/42 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DEEPFAKE DETECTION - LOCAL DESKTOP/VS CODE VERSION\n",
    "# ============================================================\n",
    "# This version is adapted for running on your local machine with GPU\n",
    "# No Google Drive mounting needed!\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import copy\n",
    "import sys\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "sys.stdout.flush()\n",
    "\n",
    "# ---------------- LOCAL PATHS CONFIG ----------------\n",
    "# CHANGE THESE PATHS TO YOUR LOCAL DIRECTORIES\n",
    "BASE_DIR = r\"C:\\Users\\Admin\\deepfake\"\n",
    "\n",
    "\n",
    "train_dirs = [\n",
    "    (os.path.join(BASE_DIR, \"train/real\"), None, 0),\n",
    "    (os.path.join(BASE_DIR, \"train/fake\"), None, 1)\n",
    "]\n",
    "valid_dirs = [\n",
    "    (os.path.join(BASE_DIR, \"valid/real\"), None, 0),\n",
    "    (os.path.join(BASE_DIR, \"valid/fake\"), None, 1)\n",
    "]\n",
    "test_image_dir = os.path.join(BASE_DIR, \"test/images\")\n",
    "test_video_dir = os.path.join(BASE_DIR, \"test/videos\")\n",
    "models_dir = os.path.join(BASE_DIR, \"models\")\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# ---------------- TRAINING CONFIG ----------------\n",
    "sequence_length = 4\n",
    "batch_size = 8  # Increase if you have good GPU (GTX 1080 or better)\n",
    "num_epochs = 25\n",
    "learning_rate = 2e-5\n",
    "patience = 4\n",
    "min_delta = 0.005\n",
    "\n",
    "# GPU Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[GPU] GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"      GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"      CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"[WARNING] No GPU detected! Training will be VERY slow on CPU.\")\n",
    "    print(\"          Consider using Google Colab if you don't have a GPU.\")\n",
    "\n",
    "# ---------------- PATH VALIDATION ----------------\n",
    "print(\"\\n[CHECK] Checking directories...\")\n",
    "required_dirs = {\n",
    "    \"Train Real\": train_dirs[0][0],\n",
    "    \"Train Fake\": train_dirs[1][0],\n",
    "    \"Valid Real\": valid_dirs[0][0],\n",
    "    \"Valid Fake\": valid_dirs[1][0],\n",
    "}\n",
    "\n",
    "all_exist = True\n",
    "for name, path in required_dirs.items():\n",
    "    exists = os.path.exists(path)\n",
    "    status = \"[OK]\" if exists else \"[MISSING]\"\n",
    "    print(f\"   {status} {name}: {path}\")\n",
    "    if exists:\n",
    "        file_count = len([f for f in os.listdir(path) if f.lower().endswith(('.jpg', '.png', '.jpeg'))])\n",
    "        print(f\"         {file_count} images\")\n",
    "    else:\n",
    "        all_exist = False\n",
    "\n",
    "if not all_exist:\n",
    "    print(\"\\n[ERROR] Some required directories are missing!\")\n",
    "    print(f\"\\nPlease create the directory structure at: {BASE_DIR}\")\n",
    "    print(\"   Deepfake/\")\n",
    "    print(\"   ├── train/real/\")\n",
    "    print(\"   ├── train/fake/\")\n",
    "    print(\"   ├── valid/real/\")\n",
    "    print(\"   └── valid/fake/\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ---------------- LOAD CLIP MODEL ----------------\n",
    "print(\"\\n[LOAD] Loading CLIP model...\")\n",
    "try:\n",
    "    clip_model_base, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        \"ViT-B-16\", pretrained=\"openai\", device=device\n",
    "    )\n",
    "    print(\"[OK] CLIP model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Error loading CLIP: {e}\")\n",
    "    print(\"Run: pip install open_clip_torch\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ---------------- TRANSFORMS ----------------\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.3),\n",
    "    preprocess.transforms[-2],\n",
    "    preprocess.transforms[-1]\n",
    "])\n",
    "test_transform = preprocess\n",
    "\n",
    "# ---------------- DATASET ----------------\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, datasets, transform=None, seq_len=8, debug=False):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        self.seq_len = seq_len\n",
    "        self.debug = debug\n",
    "        \n",
    "        real_count = 0\n",
    "        fake_count = 0\n",
    "\n",
    "        for img_dir, _, label in datasets:\n",
    "            if not os.path.exists(img_dir):\n",
    "                print(f\"[WARNING] Missing directory: {img_dir}\")\n",
    "                continue\n",
    "\n",
    "            img_files = sorted([\n",
    "                f for f in os.listdir(img_dir)\n",
    "                if os.path.isfile(os.path.join(img_dir, f)) and f.lower().endswith(('.jpg', '.png', '.jpeg'))\n",
    "            ])\n",
    "            \n",
    "            if len(img_files) < seq_len:\n",
    "                print(f\"[WARNING] Not enough images in {img_dir}: {len(img_files)} < {seq_len}\")\n",
    "                continue\n",
    "\n",
    "            for i in range(0, len(img_files) - seq_len + 1, seq_len):\n",
    "                seq_paths = [os.path.join(img_dir, img_files[j]) for j in range(i, i + seq_len)]\n",
    "                if all(os.path.exists(p) for p in seq_paths):\n",
    "                    self.samples.append((seq_paths, label))\n",
    "                    if label == 0:\n",
    "                        real_count += 1\n",
    "                    else:\n",
    "                        fake_count += 1\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"\\n[DATA] Dataset Distribution:\")\n",
    "            print(f\"       Real sequences: {real_count}\")\n",
    "            print(f\"       Fake sequences: {fake_count}\")\n",
    "            print(f\"       Total sequences: {len(self.samples)}\")\n",
    "            if real_count > 0 and fake_count > 0:\n",
    "                ratio = max(real_count, fake_count) / min(real_count, fake_count)\n",
    "                print(f\"       Class imbalance ratio: {ratio:.2f}:1\")\n",
    "                if ratio > 3:\n",
    "                    print(f\"       [WARNING] Severe class imbalance detected!\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame_paths, label = self.samples[idx]\n",
    "        frames = []\n",
    "        for p in frame_paths:\n",
    "            try:\n",
    "                img = Image.open(p).convert(\"RGB\")\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                frames.append(img)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {p}: {e}\")\n",
    "                frames.append(torch.zeros(3, 224, 224))\n",
    "        \n",
    "        frames = torch.stack(frames)\n",
    "        return frames, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# ---------------- MODEL ----------------\n",
    "class CLIP_EfficientNet_LSTM(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes=2, hidden_dim=512, lstm_layers=2, bidirectional=True, device='cpu'):\n",
    "        super(CLIP_EfficientNet_LSTM, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.clip = clip_model.visual.to(device)\n",
    "        for p in self.clip.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        effnet = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.IMAGENET1K_V1)\n",
    "        effnet = effnet.to(device).eval()\n",
    "        self.efficientnet = nn.Sequential(*list(effnet.children())[:-2])\n",
    "        self.efficientnet_avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        for p in self.efficientnet.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        clip_latent_dim = 512\n",
    "        effnet_latent_dim = 1536\n",
    "        fused_dim = clip_latent_dim + effnet_latent_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=fused_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=0.3\n",
    "        )\n",
    "        lstm_out_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LayerNorm(lstm_out_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(lstm_out_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B * T, C, H, W)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            clip_features = self.clip(x)\n",
    "            effnet_features = self.efficientnet_avgpool(self.efficientnet(x))\n",
    "            effnet_features = effnet_features.view(B * T, -1)\n",
    "\n",
    "        clip_features = clip_features.view(B, T, -1)\n",
    "        effnet_features = effnet_features.view(B, T, -1)\n",
    "        fused = torch.cat((clip_features, effnet_features), dim=2)\n",
    "\n",
    "        lstm_out, _ = self.lstm(fused)\n",
    "        x_mean = torch.mean(lstm_out, dim=1)\n",
    "        x_max, _ = torch.max(lstm_out, dim=1)\n",
    "        x = x_mean + x_max\n",
    "        \n",
    "        return self.fc(x)\n",
    "\n",
    "# ---------------- EARLY STOPPING ----------------\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=4, min_delta=0.005, mode='max'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            return False\n",
    "        \n",
    "        if self.mode == 'max':\n",
    "            improved = score > (self.best_score + self.min_delta)\n",
    "        else:\n",
    "            improved = score < (self.best_score - self.min_delta)\n",
    "        \n",
    "        if improved:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "            return self.early_stop\n",
    "\n",
    "# ---------------- LOAD DATA ----------------\n",
    "print(\"\\n[LOAD] Loading datasets...\")\n",
    "train_dataset = SequenceDataset(train_dirs, transform=train_transform, seq_len=sequence_length, debug=True)\n",
    "valid_dataset = SequenceDataset(valid_dirs, transform=test_transform, seq_len=sequence_length, debug=True)\n",
    "\n",
    "if len(train_dataset) == 0:\n",
    "    print(\"[ERROR] No training data found!\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Weighted sampling\n",
    "labels = [label for _, label in train_dataset.samples]\n",
    "class_counts = [labels.count(0), labels.count(1)]\n",
    "\n",
    "if class_counts[0] == 0 or class_counts[1] == 0:\n",
    "    print(\"[ERROR] One class has no samples!\")\n",
    "    sys.exit(1)\n",
    "\n",
    "total_samples = len(labels)\n",
    "num_classes = 2\n",
    "class_weights = [total_samples / (num_classes * count) for count in class_counts]\n",
    "sample_weights = [class_weights[l] for l in labels]\n",
    "\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# Adjust num_workers for your system (0-4 is typical for desktop)\n",
    "num_workers = 2 if os.name == 'nt' else 4  # Windows uses fewer workers\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, \n",
    "                          num_workers=num_workers, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, \n",
    "                          num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "print(f\"\\n[OK] Dataset loaded successfully\")\n",
    "print(f\"     Train batches: {len(train_loader)} | Val batches: {len(valid_loader)}\")\n",
    "print(f\"     Num workers: {num_workers}\")\n",
    "\n",
    "# ---------------- TRAINING SETUP ----------------\n",
    "print(\"\\n[BUILD] Building model...\")\n",
    "model = CLIP_EfficientNet_LSTM(clip_model_base, device=device).to(device)\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=0.1)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), \n",
    "    lr=learning_rate, \n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=2, verbose=True\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(patience=patience, min_delta=min_delta, mode='max')\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "# Training log file\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = os.path.join(models_dir, f\"training_log_{timestamp}.txt\")\n",
    "\n",
    "def log_print(message):\n",
    "    \"\"\"Print and save to log file\"\"\"\n",
    "    print(message)\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(message + '\\n')\n",
    "\n",
    "log_print(f\"\\n[TRAIN] Training Configuration:\")\n",
    "log_print(f\"        Device: {device}\")\n",
    "log_print(f\"        Max Epochs: {num_epochs}\")\n",
    "log_print(f\"        Batch Size: {batch_size}\")\n",
    "log_print(f\"        Learning Rate: {learning_rate}\")\n",
    "log_print(f\"        Patience: {patience}\")\n",
    "log_print(f\"        Log file: {log_file}\")\n",
    "log_print(\"=\" * 80)\n",
    "\n",
    "# ---------------- TRAINING LOOP ----------------\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        log_print(f\"\\n[EPOCH] {epoch+1}/{num_epochs}\")\n",
    "        log_print(\"-\" * 80)\n",
    "\n",
    "        # ============ TRAINING ============\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        class_correct = [0, 0]\n",
    "        class_total = [0, 0]\n",
    "\n",
    "        train_pbar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "\n",
    "        for batch_idx, (frames, labels_) in enumerate(train_pbar):\n",
    "            frames, labels_ = frames.to(device), labels_.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(frames)\n",
    "            loss = criterion(outputs, labels_)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels_).sum().item()\n",
    "            total += labels_.size(0)\n",
    "            \n",
    "            for i in range(len(labels_)):\n",
    "                label = labels_[i].item()\n",
    "                class_total[label] += 1\n",
    "                if preds[i] == labels_[i]:\n",
    "                    class_correct[label] += 1\n",
    "\n",
    "            current_acc = 100 * correct / total\n",
    "            train_pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{current_acc:.2f}%'\n",
    "            })\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_real_acc = 100 * class_correct[0] / class_total[0] if class_total[0] > 0 else 0\n",
    "        train_fake_acc = 100 * class_correct[1] / class_total[1] if class_total[1] > 0 else 0\n",
    "\n",
    "        # ============ VALIDATION ============\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        val_class_correct = [0, 0]\n",
    "        val_class_total = [0, 0]\n",
    "        all_probs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for frames, labels_ in tqdm(valid_loader, desc=\"Validating\", leave=False):\n",
    "                frames, labels_ = frames.to(device), labels_.to(device)\n",
    "                outputs = model(frames)\n",
    "                loss = criterion(outputs, labels_)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                val_total += labels_.size(0)\n",
    "                val_correct += (preds == labels_).sum().item()\n",
    "                \n",
    "                for i in range(len(labels_)):\n",
    "                    label = labels_[i].item()\n",
    "                    val_class_total[label] += 1\n",
    "                    if preds[i] == labels_[i]:\n",
    "                        val_class_correct[label] += 1\n",
    "                \n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "        val_acc = 100 * val_correct / val_total if val_total > 0 else 0\n",
    "        avg_val_loss = val_loss / len(valid_loader) if len(valid_loader) > 0 else 0\n",
    "        val_real_acc = 100 * val_class_correct[0] / val_class_total[0] if val_class_total[0] > 0 else 0\n",
    "        val_fake_acc = 100 * val_class_correct[1] / val_class_total[1] if val_class_total[1] > 0 else 0\n",
    "        avg_confidence = np.mean([max(p) for p in all_probs]) if all_probs else 0\n",
    "\n",
    "        scheduler.step(val_acc)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # ============ LOGGING ============\n",
    "        log_print(f\"\\n[RESULTS] Epoch {epoch+1}:\")\n",
    "        log_print(f\"          Train Loss: {avg_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        log_print(f\"            - Real: {train_real_acc:.2f}% | Fake: {train_fake_acc:.2f}%\")\n",
    "        log_print(f\"          Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "        log_print(f\"            - Real: {val_real_acc:.2f}% | Fake: {val_fake_acc:.2f}%\")\n",
    "        log_print(f\"            - Avg Confidence: {avg_confidence:.4f}\")\n",
    "        log_print(f\"          Learning Rate: {current_lr:.2e}\")\n",
    "\n",
    "        # ============ SAVE BEST MODEL ============\n",
    "        is_best = val_acc > best_val_acc\n",
    "        \n",
    "        if is_best:\n",
    "            improvement = val_acc - best_val_acc\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch + 1\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            log_print(f\"          [NEW BEST] Improvement: +{improvement:.2f}%\")\n",
    "        \n",
    "        # ============ EARLY STOPPING ============\n",
    "        if early_stopping(val_acc):\n",
    "            log_print(f\"\\n[STOP] Early stopping triggered!\")\n",
    "            log_print(f\"       No improvement for {patience} epochs\")\n",
    "            log_print(f\"       Best Val Acc: {best_val_acc:.2f}% (Epoch {best_epoch})\")\n",
    "            break\n",
    "        \n",
    "        if early_stopping.counter > 0:\n",
    "            log_print(f\"          [PATIENCE] {early_stopping.counter}/{patience}\")\n",
    "\n",
    "        if avg_confidence < 0.6:\n",
    "            log_print(f\"          [WARNING] Low average confidence ({avg_confidence:.2f})\")\n",
    "\n",
    "        log_print(\"=\" * 80)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    log_print(\"\\n[INTERRUPT] Training interrupted by user!\")\n",
    "    log_print(\"            Saving current best model...\")\n",
    "\n",
    "# ---------------- SAVE MODEL ----------------\n",
    "log_print(\"\\n[SAVE] Saving best model...\")\n",
    "model.load_state_dict(best_model_wts)\n",
    "save_path = os.path.join(models_dir, f\"clip_effnet_lstm_best_{timestamp}.pth\")\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'best_val_acc': best_val_acc,\n",
    "    'best_epoch': best_epoch,\n",
    "    'config': {\n",
    "        'sequence_length': sequence_length,\n",
    "        'hidden_dim': 512,\n",
    "        'lstm_layers': 2,\n",
    "        'bidirectional': True\n",
    "    },\n",
    "    'timestamp': timestamp\n",
    "}, save_path)\n",
    "\n",
    "log_print(f\"[OK] Model saved to: {save_path}\")\n",
    "log_print(f\"[BEST] Best Validation Accuracy: {best_val_acc:.2f}% (Epoch {best_epoch})\")\n",
    "\n",
    "# ---------------- TESTING ----------------\n",
    "if os.path.exists(test_image_dir):\n",
    "    log_print(f\"\\n[TEST] Testing on images from: {test_image_dir}\")\n",
    "    model.eval()\n",
    "    classes = [\"Real\", \"Fake\"]\n",
    "    \n",
    "    img_files = sorted([f for f in os.listdir(test_image_dir) \n",
    "                       if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "\n",
    "    if len(img_files) > 0:\n",
    "        log_print(f\"       Found {len(img_files)} test images\\n\")\n",
    "        \n",
    "        for img_file in img_files[:20]:\n",
    "            try:\n",
    "                img_path = os.path.join(test_image_dir, img_file)\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                img_tensor = test_transform(img)\n",
    "                seq_tensor = img_tensor.unsqueeze(0).repeat(sequence_length, 1, 1, 1).unsqueeze(0).to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(seq_tensor)\n",
    "                    probs = torch.softmax(outputs, dim=1)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    confidence = probs[0, preds.item()].item()\n",
    "                    \n",
    "                    if confidence > 0.75:\n",
    "                        conf_label = \"[HIGH]\"\n",
    "                    elif confidence > 0.60:\n",
    "                        conf_label = \"[MED]\"\n",
    "                    else:\n",
    "                        conf_label = \"[LOW]\"\n",
    "                    \n",
    "                    result = f\"       {conf_label} {img_file} -> {classes[preds.item()]} (conf: {confidence:.4f})\"\n",
    "                    log_print(result)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                log_print(f\"       Error processing {img_file}: {e}\")\n",
    "\n",
    "log_print(\"\\n[DONE] Training complete!\")\n",
    "log_print(f\"[LOG] Full log saved to: {log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e9f8e2-f0f1-446c-b0f4-3fcf9fe5a33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============== CONFIGURATION ==============\n",
    "BASE_DIR = r\"C:\\Users\\Admin\\deepfake\"\n",
    "fake_source = os.path.join(BASE_DIR, \"train/fake\")\n",
    "fake_output = os.path.join(BASE_DIR, \"train/fake_augmented\")\n",
    "os.makedirs(fake_output, exist_ok=True)\n",
    "\n",
    "# How many augmented versions per image\n",
    "AUGMENTATIONS_PER_IMAGE = 15\n",
    "\n",
    "# ============== ADVANCED AUGMENTATION TECHNIQUES ==============\n",
    "\n",
    "class DisasterImageAugmenter:\n",
    "    \"\"\"\n",
    "    Specialized augmentation for fake disaster image detection\n",
    "    Focuses on artifacts that AI-generated disaster images may have\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_noise(img, intensity=0.05):\n",
    "        \"\"\"Add Gaussian noise (common in AI-generated images)\"\"\"\n",
    "        img_array = np.array(img).astype(np.float32) / 255.0\n",
    "        noise = np.random.normal(0, intensity, img_array.shape)\n",
    "        noisy = np.clip(img_array + noise, 0, 1)\n",
    "        return Image.fromarray((noisy * 255).astype(np.uint8))\n",
    "    \n",
    "    @staticmethod\n",
    "    def jpeg_compression(img, quality=None):\n",
    "        \"\"\"Simulate JPEG compression artifacts\"\"\"\n",
    "        if quality is None:\n",
    "            quality = random.randint(60, 95)\n",
    "        \n",
    "        from io import BytesIO\n",
    "        buffer = BytesIO()\n",
    "        img.save(buffer, format='JPEG', quality=quality)\n",
    "        buffer.seek(0)\n",
    "        return Image.open(buffer)\n",
    "    \n",
    "    @staticmethod\n",
    "    def color_shift(img):\n",
    "        \"\"\"Shift color channels (common in fake images)\"\"\"\n",
    "        img_array = np.array(img)\n",
    "        shifts = [random.randint(-20, 20) for _ in range(3)]\n",
    "        \n",
    "        for i in range(3):\n",
    "            img_array[:, :, i] = np.clip(img_array[:, :, i].astype(np.int16) + shifts[i], 0, 255)\n",
    "        \n",
    "        return Image.fromarray(img_array.astype(np.uint8))\n",
    "    \n",
    "    @staticmethod\n",
    "    def局部模糊(img):\n",
    "        \"\"\"Local blur (simulates AI generation artifacts)\"\"\"\n",
    "        img_array = np.array(img)\n",
    "        h, w = img_array.shape[:2]\n",
    "        \n",
    "        # Random rectangular region\n",
    "        x1, y1 = random.randint(0, w//2), random.randint(0, h//2)\n",
    "        x2, y2 = random.randint(w//2, w), random.randint(h//2, h)\n",
    "        \n",
    "        # Blur that region\n",
    "        region = img_array[y1:y2, x1:x2]\n",
    "        blurred = cv2.GaussianBlur(region, (15, 15), 0)\n",
    "        img_array[y1:y2, x1:x2] = blurred\n",
    "        \n",
    "        return Image.fromarray(img_array)\n",
    "    \n",
    "    @staticmethod\n",
    "    def edge_enhancement(img):\n",
    "        \"\"\"Enhance edges (fake images often have weird edges)\"\"\"\n",
    "        enhancer = ImageEnhance.Sharpness(img)\n",
    "        factor = random.uniform(1.5, 3.0)\n",
    "        return enhancer.enhance(factor)\n",
    "    \n",
    "    @staticmethod\n",
    "    def chromatic_aberration(img):\n",
    "        \"\"\"Simulate chromatic aberration\"\"\"\n",
    "        img_array = np.array(img)\n",
    "        h, w = img_array.shape[:2]\n",
    "        \n",
    "        # Slight shift in color channels\n",
    "        shift = random.randint(1, 3)\n",
    "        r = img_array[:, :, 0]\n",
    "        g = img_array[:, :, 1]\n",
    "        b = img_array[:, :, 2]\n",
    "        \n",
    "        # Shift channels\n",
    "        r = np.roll(r, shift, axis=1)\n",
    "        b = np.roll(b, -shift, axis=1)\n",
    "        \n",
    "        img_array[:, :, 0] = r\n",
    "        img_array[:, :, 2] = b\n",
    "        \n",
    "        return Image.fromarray(img_array)\n",
    "    \n",
    "    @staticmethod\n",
    "    def contrast_adjustment(img):\n",
    "        \"\"\"Random contrast adjustment\"\"\"\n",
    "        enhancer = ImageEnhance.Contrast(img)\n",
    "        factor = random.uniform(0.7, 1.5)\n",
    "        return enhancer.enhance(factor)\n",
    "    \n",
    "    @staticmethod\n",
    "    def saturation_adjustment(img):\n",
    "        \"\"\"Random saturation adjustment\"\"\"\n",
    "        enhancer = ImageEnhance.Color(img)\n",
    "        factor = random.uniform(0.6, 1.8)\n",
    "        return enhancer.enhance(factor)\n",
    "    \n",
    "    @staticmethod\n",
    "    def rotation_small(img):\n",
    "        \"\"\"Small rotation\"\"\"\n",
    "        angle = random.uniform(-15, 15)\n",
    "        return img.rotate(angle, fillcolor=(128, 128, 128), expand=False)\n",
    "    \n",
    "    @staticmethod\n",
    "    def perspective_warp(img):\n",
    "        \"\"\"Perspective transformation\"\"\"\n",
    "        img_array = np.array(img)\n",
    "        h, w = img_array.shape[:2]\n",
    "        \n",
    "        # Random perspective points\n",
    "        margin = int(0.1 * min(w, h))\n",
    "        src_points = np.float32([[0, 0], [w, 0], [w, h], [0, h]])\n",
    "        dst_points = np.float32([\n",
    "            [random.randint(0, margin), random.randint(0, margin)],\n",
    "            [w - random.randint(0, margin), random.randint(0, margin)],\n",
    "            [w - random.randint(0, margin), h - random.randint(0, margin)],\n",
    "            [random.randint(0, margin), h - random.randint(0, margin)]\n",
    "        ])\n",
    "        \n",
    "        matrix = cv2.getPerspectiveTransform(src_points, dst_points)\n",
    "        warped = cv2.warpPerspective(img_array, matrix, (w, h))\n",
    "        return Image.fromarray(warped)\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_weather_effect(img):\n",
    "        \"\"\"Simulate weather effects (rain, fog, etc.)\"\"\"\n",
    "        img_array = np.array(img).astype(np.float32)\n",
    "        \n",
    "        effect_type = random.choice(['fog', 'brightness'])\n",
    "        \n",
    "        if effect_type == 'fog':\n",
    "            # Add fog/haze\n",
    "            fog_intensity = random.uniform(0.2, 0.5)\n",
    "            fog = np.ones_like(img_array) * 200\n",
    "            img_array = img_array * (1 - fog_intensity) + fog * fog_intensity\n",
    "        \n",
    "        elif effect_type == 'brightness':\n",
    "            # Adjust brightness\n",
    "            brightness = random.uniform(0.8, 1.3)\n",
    "            img_array = img_array * brightness\n",
    "        \n",
    "        img_array = np.clip(img_array, 0, 255).astype(np.uint8)\n",
    "        return Image.fromarray(img_array)\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_random_augmentations(img, num_augs=3):\n",
    "        \"\"\"Apply multiple random augmentations\"\"\"\n",
    "        augmentations = [\n",
    "            DisasterImageAugmenter.add_noise,\n",
    "            DisasterImageAugmenter.jpeg_compression,\n",
    "            DisasterImageAugmenter.color_shift,\n",
    "            DisasterImageAugmenter.局部模糊,\n",
    "            DisasterImageAugmenter.edge_enhancement,\n",
    "            DisasterImageAugmenter.chromatic_aberration,\n",
    "            DisasterImageAugmenter.contrast_adjustment,\n",
    "            DisasterImageAugmenter.saturation_adjustment,\n",
    "            DisasterImageAugmenter.rotation_small,\n",
    "            DisasterImageAugmenter.perspective_warp,\n",
    "            DisasterImageAugmenter.add_weather_effect\n",
    "        ]\n",
    "        \n",
    "        selected_augs = random.sample(augmentations, min(num_augs, len(augmentations)))\n",
    "        \n",
    "        for aug in selected_augs:\n",
    "            try:\n",
    "                img = aug(img)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARNING] Augmentation failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return img\n",
    "\n",
    "\n",
    "# ============== MAIN AUGMENTATION LOOP ==============\n",
    "\n",
    "def augment_dataset():\n",
    "    \"\"\"Generate augmented versions of all fake images\"\"\"\n",
    "    \n",
    "    print(f\"[START] Augmenting fake disaster images...\")\n",
    "    print(f\"        Source: {fake_source}\")\n",
    "    print(f\"        Output: {fake_output}\")\n",
    "    print(f\"        Augmentations per image: {AUGMENTATIONS_PER_IMAGE}\")\n",
    "    \n",
    "    # Get all fake images\n",
    "    fake_images = [f for f in os.listdir(fake_source) \n",
    "                   if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    if len(fake_images) == 0:\n",
    "        print(\"[ERROR] No images found in fake source directory!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n[INFO] Found {len(fake_images)} fake images\")\n",
    "    print(f\"[INFO] Will generate {len(fake_images) * AUGMENTATIONS_PER_IMAGE} total images\\n\")\n",
    "    \n",
    "    augmenter = DisasterImageAugmenter()\n",
    "    total_generated = 0\n",
    "    \n",
    "    for img_file in tqdm(fake_images, desc=\"Augmenting\"):\n",
    "        try:\n",
    "            img_path = os.path.join(fake_source, img_file)\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            # Save original\n",
    "            base_name = os.path.splitext(img_file)[0]\n",
    "            img.save(os.path.join(fake_output, f\"{base_name}_original.jpg\"))\n",
    "            total_generated += 1\n",
    "            \n",
    "            # Generate augmented versions\n",
    "            for i in range(AUGMENTATIONS_PER_IMAGE):\n",
    "                # Apply 2-4 random augmentations\n",
    "                num_augs = random.randint(2, 4)\n",
    "                aug_img = augmenter.apply_random_augmentations(img.copy(), num_augs)\n",
    "                \n",
    "                # Save augmented image\n",
    "                output_name = f\"{base_name}_aug_{i:03d}.jpg\"\n",
    "                aug_img.save(os.path.join(fake_output, output_name), quality=95)\n",
    "                total_generated += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\n[ERROR] Failed to process {img_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n[DONE] Generated {total_generated} images\")\n",
    "    print(f\"[INFO] Original images: {len(fake_images)}\")\n",
    "    print(f\"[INFO] Augmented images: {total_generated - len(fake_images)}\")\n",
    "    print(f\"\\n[NEXT STEP] Update your training script:\")\n",
    "    print(f\"            train_dirs = [\")\n",
    "    print(f\"                (r'{BASE_DIR}\\\\train\\\\real', None, 0),\")\n",
    "    print(f\"                (r'{fake_output}', None, 1)  # Use augmented fake images\")\n",
    "    print(f\"            ]\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    augment_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2993001b-efad-41d5-9444-90c748afbccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
